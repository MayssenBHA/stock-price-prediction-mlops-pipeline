# -*- coding: utf-8 -*-
"""Att_CNN_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYaRMCAi7_K3GAJuLFWxSjTSvvw4V6vJ
"""

from keras.optimizers import Adam
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn import metrics
from keras.layers import Input, Permute, Dense, Lambda, RepeatVector, Multiply, Conv1D, Dropout, Bidirectional, LSTM, Flatten
from keras.models import Model
import tensorflow as tf
from tensorflow.keras import backend as K
# from utils import *
# from model import *
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

"""Load Data"""

data1 = pd.read_csv("vgi2.csv")
# data1.index = pd.to_datetime(data1['date'], format='%Y-%m-%d')
data1.head()

data1 = data1.iloc[1:, :]
data1.head()

"""Load ARIMA residuals"""

import statsmodels.api as sm
model = sm.tsa.ARIMA(endog=data1['close'], order=(0, 1, 0)).fit()
residuals = pd.DataFrame(model.resid)

residuals.head()  # mất ngày 25

"""Merge Residual with date, close, open, high, low, nmVolume"""

data1 = pd.merge(data1, residuals, on=data1['date'])  # feature engineering bằng cách lấy biến phần dư của mô hình ARIMA
data1.head()

data1.pop('key_0')
data1.set_index('date', inplace=True)

data1.head()

"""Spliting Train - Validation - Test"""

# data = data1.iloc[1:808, :] # train
# data2 = data1.iloc[808:, :] # test
train = data1[data1.index <= '2020-11-24']
valid = data1[(data1.index <= '2021-10-29')&(data1.index > '2020-11-24')]
test_set = data1[(data1.index <= '2021-12-31')&(data1.index > '2021-10-29')]
print('train shape:', train.shape)
print('validation shape:', valid.shape)
print('test shape:', test_set.shape)

training_set = pd.concat([train, valid], axis=0)
training_set

"""Min Max Scale"""

from sklearn.preprocessing import MinMaxScaler

# Convert column names to strings
training_set.columns = training_set.columns.astype(str)
valid.columns = valid.columns.astype(str)
test_set.columns = test_set.columns.astype(str)

data_sc = MinMaxScaler()
train_scaled = data_sc.fit_transform(training_set)
valid_scaled = data_sc.transform(valid)
test_set_scaled = data_sc.transform(test_set)

TIME_STEPS = 6


train_close_scaled = train_scaled[:, 0].reshape(len(training_set), 1) # 0 means close
valid_close_scaled = valid_scaled[:, 0].reshape(len(valid), 1) # 0 means close
test_close_scaled = test_set_scaled[:, 0].reshape(len(test_set), 1) # 0 means close

def create_dataset(dataset, look_back=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-look_back):
        a = dataset[i:(i+look_back),:]
        dataX.append(a)
        dataY.append(dataset[i + look_back,:])
    TrainX = np.array(dataX)
    Train_Y = np.array(dataY)

    return TrainX, Train_Y

"""Convert Supervised Data"""

train_X, _ = create_dataset(train_scaled, TIME_STEPS)
_, train_Y = create_dataset(train_close_scaled, TIME_STEPS) # biến dự đoán close

print(train_X.shape, train_Y.shape)

valid_X, _ = create_dataset(valid_scaled, TIME_STEPS)
_, valid_Y = create_dataset(valid_close_scaled, TIME_STEPS) # biến dự đoán close

print(valid_X.shape, valid_Y.shape)

test_X, _ = create_dataset(test_set_scaled, TIME_STEPS)
_, test_Y = create_dataset(test_close_scaled, TIME_STEPS) # biến dự đoán close

print(test_X.shape, test_Y.shape)

"""Building Attention-Based CNN-LSTM architecture"""



def attention_3d_block(inputs, single_attention_vector=False):
    # inputs.shape = (batch_size, time_steps, input_dim)
    time_steps = tf.keras.backend.int_shape(inputs)[1]
    input_dim = tf.keras.backend.int_shape(inputs)[2]
    a = Permute((2, 1))(inputs)
    a = Dense(time_steps, activation='softmax')(a)
    if single_attention_vector:
        a = Lambda(lambda x: K.mean(x, axis=1))(a)
        a = RepeatVector(input_dim)(a)

    a_probs = Permute((2, 1))(a)
    # element-wise
    output_attention_mul = Multiply()([inputs, a_probs])
    return output_attention_mul

def attention_model(INPUT_DIMS = 6,TIME_STEPS = 1,lstm_units = 50): #64
    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))
    # Input()` is used to instantiate a Keras tensor.

    x = Conv1D(filters=50, kernel_size=1, activation='relu')(inputs)  # padding = 'same', filters=64
    x = Dropout(0.2)(x) #0.2

    # lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)
    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)
    # lstm_out = LSTM(lstm_units, return_sequences=True)(x)
    lstm_out = Dropout(0.2)(lstm_out) #0.2
    attention_mul = attention_3d_block(lstm_out)
    attention_mul = Flatten()(attention_mul)  # lấy tích reshape

    output = Dense(1, activation='sigmoid')(attention_mul)
    model = Model(inputs=[inputs], outputs=output)
    return model

"""#Training and Save Model"""

m = attention_model(INPUT_DIMS=6, TIME_STEPS = 6, lstm_units = 50)  #7 number of inputs feature, our data just have 6
m.summary()
adam = Adam(learning_rate=0.01)
m.compile(optimizer=adam, loss='mse')
history = m.fit([train_X], train_Y, epochs=50, batch_size=32, validation_split=0.3)  # validation, 0.3,
# history = m.fit([train_X], train_Y, epochs=50, batch_size=32, validation_data=(valid_X, valid_Y))

# X_train: data 5 biến + phần ARIMA redidual
m.save("./stock_model.h5")
np.save("stock_normalize.npy", data_sc)

# Calculate train min/max for inverse scaling
train_min = training_set.min(axis=0)[0]
train_max = training_set.max(axis=0)[0]

# Save scaler as pickle
import pickle
with open('stock_scaler.pkl', 'wb') as f:
    pickle.dump(data_sc, f)

# Save model config as pickle
model_config = {
    'train_min': float(train_min),
    'train_max': float(train_max),
    'TIME_STEPS': TIME_STEPS,
    'INPUT_DIMS': 6,
    'lstm_units': 50
}
with open('model_config.pkl', 'wb') as f:
    pickle.dump(model_config, f)

print("Models saved successfully!")
print("- stock_model.h5 (Keras weights)")
print("- stock_scaler.pkl (MinMaxScaler)")
print("- model_config.pkl (Configuration)")

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.show()

# time_steps = 1
# lstm_unit = 64. loss: 0.0027 - val_loss: 0.0017, on train scaled
# lstm_unit = 50, cnn_unit = 64. loss: 0.0020 - val_loss: 0.0111
# lstm_unit = 64, cmm_unit = 64 loss: 0.0018 - val_loss: 0.0106
# lstm_unit = 50, cnn_unit = 50 loss: 0.0016 - val_loss: 0.0102

# time_steps = 6
# lstm_unit = 50, cnn_unit = 50 loss: 0.0014 - val_loss: 0.0099

# time_steps = 10
# lstm_unit = 50, cnn_unit = 50 loss: 0.0015 - val_loss: 0.0116

"""#Load model for prediction"""

model = attention_model(INPUT_DIMS=6, TIME_STEPS = 6, lstm_units = 50)
model.load_weights('stock_model.h5')
model.summary()
y_hat_test =  model.predict(test_X)

y_hat_train =  model.predict(train_X)
y_hat_valid =  model.predict(valid_X)

"""Inverse Min Max Scale"""

train_min = training_set.min(axis=0)[0]
train_max = training_set.max(axis=0)[0]
print(train_min)
print(train_max)

# Train set
y_hat_train_unscaled = y_hat_train*(train_max - train_min) + train_min
y_train_unscaled = train_Y*(train_max - train_min) + train_min

# Valid set
y_hat_valid_unscaled = y_hat_valid*(train_max - train_min) + train_min
y_valid_unscaled = valid_Y*(train_max - train_min) + train_min

# Test set
y_hat_test_unscaled = y_hat_test*(train_max - train_min) + train_min
y_test_unscaled = test_Y*(train_max - train_min) + train_min

"""Evaluate Performance"""

def evaluation_metric(y_test,y_hat):
    MSE = metrics.mean_squared_error(y_test, y_hat)
    RMSE = MSE**0.5
    MAE = metrics.mean_absolute_error(y_test,y_hat)
    R2 = metrics.r2_score(y_test,y_hat)
    print('MSE: %.5f' % MSE)
    print('RMSE: %.5f' % RMSE)
    print('MAE: %.5f' % MAE)
    print('R2: %.5f' % R2)

def GetMAPE(y_hat, y_test):
    sum = np.mean(np.abs((y_hat - y_test) / y_test)) * 100
    return sum

print('training evaluate')
y_hat_train_unscaled = np.array(y_hat_train_unscaled, dtype='float64')
y_train_unscaled = np.array(y_train_unscaled, dtype='float64')

evaluation_metric(y_train_unscaled, y_hat_train_unscaled)
# def GetMAPE(y_hat, y_test):
print("MAPE:", GetMAPE(y_hat_train_unscaled, y_train_unscaled), '%')

print('valiation evaluate')

y_hat_valid_unscaled = np.array(y_hat_valid_unscaled, dtype='float64')
y_valid_unscaled = np.array(y_valid_unscaled, dtype='float64')

evaluation_metric(y_valid_unscaled, y_hat_valid_unscaled)
print("MAPE:", GetMAPE(y_hat_valid_unscaled, y_valid_unscaled), '%')

print('test evaluate')

y_hat_test_unscaled = np.array(y_hat_test_unscaled, dtype='float64')
y_test_unscaled = np.array(y_test_unscaled, dtype='float64')

evaluation_metric(y_test_unscaled, y_hat_test_unscaled)
print("MAPE:", GetMAPE(y_hat_test_unscaled, y_test_unscaled), '%')

time_train = pd.to_datetime(training_set.index)
TIME_STEPS = 6
time_train = time_train[TIME_STEPS:]


plt.figure(figsize=(20, 5))
plt.plot(time_train, training_set.iloc[TIME_STEPS:, 0], label='Stock Price')
plt.plot(time_train, y_hat_train_unscaled, label='Predicted Stock Price')
plt.title(f'Attention-Based CNN-LSTM: Stock Price Prediction on Training set')
plt.xlabel('Time', fontsize=12, verticalalignment='top')
plt.ylabel('Close', fontsize=14, horizontalalignment='center')
plt.legend()
plt.show()

time_valid = pd.to_datetime(valid.index)
TIME_STEPS = 6
time_valid = time_valid[TIME_STEPS:]


plt.figure(figsize=(20, 5))
plt.plot(time_valid, valid.iloc[TIME_STEPS:, 0], label='Stock Price')
plt.plot(time_valid, y_hat_valid_unscaled, label='Predicted Stock Price')
plt.title(f'Attention-Based CNN-LSTM: Stock Price Prediction on Validation set')
plt.xlabel('Time', fontsize=12, verticalalignment='top')
plt.ylabel('Close', fontsize=14, horizontalalignment='center')
plt.legend()
plt.show()

time_test = pd.to_datetime(test_set.index)
TIME_STEPS = 6
time_test = time_test[TIME_STEPS:]
time_test

plt.figure(figsize=(20, 5))
plt.plot(time_test, test_set.iloc[TIME_STEPS:, 0], label='Stock Price')
plt.plot(time_test, y_hat_test_unscaled, label='Predicted Stock Price')
plt.title(f'Attention-Based CNN-LSTM: Stock Price Prediction on Test set')
plt.xlabel('Time', fontsize=12, verticalalignment='top')
plt.ylabel('Close', fontsize=14, horizontalalignment='center')
plt.legend()
plt.show()